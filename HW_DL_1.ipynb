{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Написать на PyTorch глубокую сеть. Проверить форвард пасса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLDGwrpoTPA8",
    "outputId": "3eb466c8-7029-440d-f1e7-8093cad340af"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(1, 100)\n",
    "        self.activ = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(100, 1)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.do(x)\n",
    "        x = self.layer2(x)\n",
    "                        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (torch.rand(1000)-0.4)*3\n",
    "y = x**3 + torch.randn(1)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model.train() \n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\150ho\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 999, loss: 0.16550229489803314\n",
      "epoch: 1, step: 999, loss: 0.12685929238796234\n",
      "epoch: 2, step: 999, loss: 0.061052531003952026\n",
      "epoch: 3, step: 999, loss: 0.0025708989705890417\n",
      "epoch: 4, step: 999, loss: 0.00047343346523121\n",
      "epoch: 5, step: 999, loss: 0.004774009808897972\n",
      "epoch: 6, step: 999, loss: 0.033541977405548096\n",
      "epoch: 7, step: 999, loss: 0.0007575800991617143\n",
      "epoch: 8, step: 999, loss: 0.009023685939610004\n",
      "epoch: 9, step: 999, loss: 0.011115718632936478\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (val, t) in enumerate(zip(x, y)):\n",
    "        optim.zero_grad()\n",
    "        predict = model(val.unsqueeze(dim=0))\n",
    "        loss = loss_func(predict, t)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Написать адаптивный оптимизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KmLRIBk4Fz12"
   },
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# Realize SGD Momentum optimizer\n",
    "# velocity = momentum * velocity - lr * gradient\n",
    "# w = w + velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс линейный слой с леккции №1\n",
    "class LinearLayer:\n",
    "    def __init__(self, n_inp, n_out, activation='sigmoid'):\n",
    "        self.w = np.random.randn(n_out, n_inp) * 0.1\n",
    "        self.b = np.random.randn(n_out, 1) * 0.1\n",
    "        if activation == 'sigmoid':\n",
    "            self.activ = sigmoid\n",
    "        if activation == 'relu':\n",
    "            self.activ = relu\n",
    "        elif activation == 'None':\n",
    "            self.activ = None\n",
    "        else:\n",
    "            raise Exception(f'Unknown activation \"{activation}\"')\n",
    "        self._clear_state()\n",
    "\n",
    "    def _clear_state(self):\n",
    "        self.lin = None\n",
    "        self.inp = None\n",
    "        self.d_w = None\n",
    "        self.d_b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.inp = x\n",
    "        self.lin = np.dot(self.w, x) + self.b\n",
    "        activ = self.activ(self.lin) if self.activ is not None else self.lin\n",
    "\n",
    "        return activ\n",
    "\n",
    "    def backward(self, grad): # grad = d L / d z    Dout \n",
    "        # grad * dz / d lin\n",
    "        if self.activ == sigmoid:\n",
    "            grad_lin = sigmoid_backward(grad, self.lin) \n",
    "        elif self.activ == relu:\n",
    "            grad_lin = relu_backward(grad, self.lin)\n",
    "        else:\n",
    "            grad_lin = grad\n",
    "        # grad_lin * d lin / d w \n",
    "        m = self.inp.shape[1]\n",
    "        self.d_w = np.dot(grad_lin, self.inp.T) / m    # d_in dOut\n",
    "        # grad_lin * d lin / d b \n",
    "        self.d_b = np.sum(grad_lin, axis=1, keepdims=True) / m\n",
    "\n",
    "        grad = np.dot(self.w.T, grad_lin)\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rkj0NKbQWX_9"
   },
   "outputs": [],
   "source": [
    "#для одного слоя (оптимицатор с лекции №1)\n",
    "class SGDMomentum1:\n",
    "    def __init__(self, model: LinearLayer, lr=0.001, momentum=0.99):\n",
    "        self.lr = lr\n",
    "        self.m = momentum\n",
    "        self.model = model\n",
    "\n",
    "        self.vel_w = np.zeros_like(model.w)\n",
    "        self.vel_b = np.zeros_like(model.b)\n",
    "\n",
    "    def step(self):\n",
    "        self.vel_w = self.m * self.vel_w - self.lr * self.model.d_w\n",
    "        self.vel_b = self.m * self.vel_b - self.lr * self.model.d_b\n",
    "\n",
    "        self.model.w += self.vel_w\n",
    "        self.model.b += self.vel_b\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.model.d_w = np.zeros_like(self.model.d_w)\n",
    "        self.model.d_b = np.zeros_like(self.model.d_b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Оптимизатор Адам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonguides.com/adam-optimizer-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/ru/company/skillfactory/blog/525214/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-implement-an-adam-optimizer-from-scratch-76e7b217f1cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptim():\n",
    "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.m_dw, self.v_dw = 0, 0\n",
    "        self.m_db, self.v_db = 0, 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "    def update(self, t, w, b, dw, db):\n",
    "        ## dw, db из текущей минибатча\n",
    "        ## momentum beta 1\n",
    "        # *** веса *** #\n",
    "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
    "        # *** смещения *** #\n",
    "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
    "\n",
    "        ## rms beta 2\n",
    "        # *** веса *** #\n",
    "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
    "        # *** смещения *** #\n",
    "        self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db)\n",
    "\n",
    "        ## коррекция смещения\n",
    "        m_dw_corr = self.m_dw/(1-self.beta1**t)\n",
    "        m_db_corr = self.m_db/(1-self.beta1**t)\n",
    "        v_dw_corr = self.v_dw/(1-self.beta2**t)\n",
    "        v_db_corr = self.v_db/(1-self.beta2**t)\n",
    "\n",
    "        ## обновить веса и смещения\n",
    "        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
    "        b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Решить задачу нахождения корней квадратного уравнения методом градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_5jAO8pVFz19"
   },
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# Find the roots of square equation by gradient descent\n",
    "# x ** 2 - 6 * x + 4 = 0\n",
    "\n",
    "# посчитать производную от преобразованной функции\n",
    "# надо начать движение от начальной точки в направлении антиградиента с заданным шагом\n",
    "# x = x - lr * grad(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ru.wikipedia.org/wiki/Дискриминант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_EkHxWgVTPip"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.23606797749979 0.7639320225002102\n"
     ]
    }
   ],
   "source": [
    "# Найдем корни аналитически (через дискриминант)\n",
    "# x ** 2 - 6 * x + 4 = 0\n",
    "a = 1\n",
    "b = -6\n",
    "c = 4\n",
    "D = b**2 - 4*a*c\n",
    "x1 = (-b + D**0.5)/(2*a)\n",
    "x2 = (-b - D**0.5)/(2*a)\n",
    "print(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.programmersought.com/article/16521452441/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dmitrymakarov.ru/opt/gradient-02/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/if-we-have-any-function-and-we-want-find-the-extremum-of-that-function-whether-it-is-minima-or-3dd53d89ea40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заданная функция\n",
    "def f1(x):\n",
    "    return x**2 - 6*x + 4    \n",
    "\n",
    "# производная функции\n",
    "def grad1(x):\n",
    "    return 2 * x - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.763948124793057, 88011), (5.236067487546305, 57059))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# метод градиентного спуска \n",
    "def gradient_descent(x, lr, acc):\n",
    "    x1 = x\n",
    "    x2 = x1 - lr * grad1(x1)\n",
    "    count = 1\n",
    "    while f1(x2) > acc:\n",
    "        x1 = x2\n",
    "        x2 = x1 - lr * grad1(x1)\n",
    "        count += 1\n",
    "    return x2, count\n",
    "    \n",
    "gradient_descent(-10, 0.00001, 0.00001), gradient_descent(10, 0.00001, 0.00001)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция в квадрате\n",
    "def f2(x):\n",
    "    return (x**2 - 6*x + 4)**2     # x**4 - 12 * x**3 + 44*x**2 - 48 * x + 16\n",
    "\n",
    "# производная функции в квадрате\n",
    "def grad2(x):\n",
    "    return 2 * (x ** 2 - 6 * x + 4) * (2 * x - 6)  # 4 * x**3 - 36 * x**2 + 88 * x - 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7632443126002381, 174), (5.23677368251635, 176))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# метод Ньютона\n",
    "def newton(x, lr, acc):\n",
    "    x1 = x\n",
    "    x2 = x1 - lr * grad2(x1)\n",
    "    count = 1\n",
    "    while f2(x2) > acc:\n",
    "        x1 = x2\n",
    "        x2 = x1 - lr * grad2(x1)\n",
    "        count += 1\n",
    "    return x2, count\n",
    "\n",
    "newton(-10, 0.001, 0.00001), newton(10, 0.001, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводы:\n",
    "# всегда ли сойдемся за приемлемое количество шагов? - Количество шагов зависит от LR и заданной точности (acc)\n",
    "# важна ли начальная точка? - От начальной точки зависит в какой минимум будет производиться спуск\n",
    "# как найти второй корень? - Выбрать другую начальную точку (шагать с дыух шагов)\n",
    "# как вляет ЛР? - ЛР влияет на скорость спуска (количество шагов)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
